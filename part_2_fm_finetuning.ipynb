{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cbc75b",
   "metadata": {},
   "source": [
    "# Часть 2. Файнтюнинг диффузионных моделей <br> (2 балла)\n",
    "\n",
    "Вам предстоит взять обученную text-to-image диффузионную модель [Stable Diffusion 1.5](https://huggingface.co/sd-legacy/stable-diffusion-v1-5), дообучить её в Rectified Flow (RF) режиме по мотивам [Stable Diffusion 3](https://arxiv.org/pdf/2403.03206) и привести результаты с помощью любого солвера и расписания, реализованного в первой части задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aff8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U diffusers --upgrade\n",
    "!git clone https://github.com/quickjkee/ysda_hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict, set_peft_model_state_dict\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b73147",
   "metadata": {},
   "source": [
    "### Utils\n",
    "Класс датасета и доп. функции, которые потребуются позже. Можно сюда не смотреть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------    \n",
    "# Visualization utils \n",
    "#---------------------\n",
    "\n",
    "def visualize_images(images):\n",
    "    assert len(images) == 4\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=-0.01, hspace=-0.01)\n",
    "    \n",
    "    \n",
    "#--------------    \n",
    "# Tensor utils \n",
    "#--------------\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "#---------------\n",
    "# Dataset utils\n",
    "#---------------\n",
    "\n",
    "class TextImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, subset_name, transform=None, max_cnt=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.extensions = (\n",
    "            \".jpg\",\n",
    "            \".jpeg\",\n",
    "            \".png\",\n",
    "            \".ppm\",\n",
    "            \".bmp\",\n",
    "            \".pgm\",\n",
    "            \".tif\",\n",
    "            \".tiff\",\n",
    "            \".webp\",\n",
    "        )\n",
    "        sample_dir = os.path.join(root_dir, subset_name)\n",
    "\n",
    "        # Collect sample paths\n",
    "        self.samples = sorted(\n",
    "            [\n",
    "                os.path.join(sample_dir, fname)\n",
    "                for fname in os.listdir(sample_dir)\n",
    "                if fname[-4:] in self.extensions\n",
    "            ],\n",
    "            key=lambda x: x.split(\"/\")[-1].split(\".\")[0],\n",
    "        )\n",
    "        self.samples = (\n",
    "            self.samples if max_cnt is None else self.samples[:max_cnt]\n",
    "        )  # restrict num samples\n",
    "\n",
    "        # Collect captions\n",
    "        self.captions = {}\n",
    "        with open(\n",
    "            os.path.join(root_dir, f\"{subset_name}.csv\"), newline=\"\\n\"\n",
    "        ) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=\",\")\n",
    "            for i, row in enumerate(spamreader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                self.captions[row[1]] = row[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample_path = self.samples[idx]\n",
    "        sample = Image.open(sample_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return {\n",
    "            \"image\": sample,\n",
    "            \"text\": self.captions[os.path.basename(sample_path)],\n",
    "            \"idxs\": idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcb50b",
   "metadata": {},
   "source": [
    "## Базовая модель (SD1.5)\n",
    "\n",
    "Для начала загрузим модель [StableDiffusion 1.5](https://huggingface.co/sd-legacy/stable-diffusion-v1-5) и сгенерируем ей картинки за 5, 10 и 20 шагов.\n",
    "\n",
    "**Важно:** для экономии памяти, загружаем все компоненты модели в FP16. Не забываем положить модель на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc787c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = <YOUR CODE HERE>\n",
    "\n",
    "# Проверяем, что все компоненты модели в FP16 и на cuda\n",
    "assert pipe.unet.dtype == torch.float16 and pipe.unet.device.type == 'cuda'\n",
    "assert pipe.vae.dtype == torch.float16 and pipe.vae.device.type == 'cuda'\n",
    "assert pipe.text_encoder.dtype == torch.float16 and pipe.text_encoder.device.type == 'cuda'\n",
    "\n",
    "# Заменяем дефолтный сэмплер на DDIM\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "pipe.scheduler.timesteps = pipe.scheduler.timesteps.cuda()\n",
    "pipe.scheduler.alphas_cumprod = pipe.scheduler.alphas_cumprod.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba745e5",
   "metadata": {},
   "source": [
    "Вызовите pipe для данного промпта, передав ему число шагов, гайденс скейл и генератор случайных чисел. На промпт генерируем 4 картинки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ebd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A sad puppy with large eyes\"\n",
    "guidance_scale = 7.5\n",
    "\n",
    "for num_inference_steps in [5, 10, 20]:\n",
    "    generator = torch.Generator('cuda').manual_seed(0)\n",
    "    \n",
    "    images = <YOUR CODE HERE>\n",
    "    \n",
    "    visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495875f",
   "metadata": {},
   "source": [
    "## Подготовка датасета\n",
    "\n",
    "Мы будем дообучать модель на небольшой обучающей выборке из 10000 пар текст-картинка сгенерированные моделью [FLUX](https://huggingface.co/black-forest-labs/FLUX.1-dev).\n",
    "\n",
    "Данные можно загрузить с помощью команд в ячейке ниже. В текущей директории ./ должны появиться:\n",
    "* Папка flux_data с 10000 картинками\n",
    "* Файл flux_data.csv с 10000 промптами\n",
    "\n",
    "Данные парсятся корректным образом в уже реализованном классе **TextImageDataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.yandexcloud.net/yandex-research/flux_data_10k.tar.gz\n",
    "!tar -xzf flux_data_10k.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ada47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(512),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 2 * x - 1,\n",
    "    ]\n",
    ")\n",
    "dataset = TextImageDataset(\".\",\n",
    "    subset_name=\"flux_data\",\n",
    "    transform=transform,\n",
    "    max_cnt=5000, # Можно варьировать, если очень долго учится\n",
    ")\n",
    "assert len(dataset) == 5000 #10000\n",
    "\n",
    "batch_size = 8 # Рекоммендуемый размер батча на Colab\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, shuffle=True, batch_size=batch_size, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759401e",
   "metadata": {},
   "source": [
    "## Дообучение SD1.5 в Rectified Flow режиме\n",
    "\n",
    "Функции ниже используются для извлечения эмбедингов из текстовых промптов и перевода картинок в латентное пространство VAE. \n",
    "\n",
    "### Задание 1 (0.5 балла)\n",
    "\n",
    "* Предпосчитать эмбеддинги для пустого промпта \"\". Будет использоваться для обучения с CFG.\n",
    "* В функции **prepare_batch** вам нужно замиксовать **uncond_prompt_embeds** в батч с заданной вероятностью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем эмбеддинги для пустого текста для CFG обучения\n",
    "# Можно подглядеть в ячейку ниже\n",
    "\n",
    "uncond_prompt_embeds = <YOUR CORE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_batch(batch, pipe, uncond_prob=0.1):\n",
    "    \"\"\"\n",
    "    Предобработка батча картинок и текстовых промптов.\n",
    "    Маппим картинки в латентное пространство VAE.\n",
    "    Извлекаем эмбеды промптов с помощью текстового энкодера.\n",
    "    \n",
    "    Params:\n",
    "\n",
    "    Return:\n",
    "        latents: torch.Tensor([B, 4, 64, 64], dtype=torch.float16)\n",
    "        prompt_embeds: torch.Tensor([B, 77, D], dtype=torch.float16)\n",
    "    \"\"\"\n",
    "\n",
    "    # Токенизируем промпты\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Извлекаем эмбеды промптов с помощью текстового энкодера\n",
    "    prompt_embeds = pipe.text_encoder(text_inputs.input_ids.cuda())[0]\n",
    "    \n",
    "    # Замешиваем в батч uncond_prompt_embeds c вероятностью uncond_prob\n",
    "    <YOUR_CODE_HERE>\n",
    "    \n",
    "    # Переводим картинки в латентное пространство VAE\n",
    "    image = batch['image'].to(\"cuda\", dtype=torch.float16)\n",
    "    latents = pipe.vae.encode(image).latent_dist.sample()\n",
    "    latents = latents * pipe.vae.config.scaling_factor\n",
    "    return latents, prompt_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba281e97",
   "metadata": {},
   "source": [
    "## Подготовка модели\n",
    "\n",
    "Для экономии памяти во время обучения будем учить не параметры самой модели, а добавим в нее обучаемые LoRA адаптеры с малым числом параметров.\n",
    "\n",
    "LoRA представляет собой маленькую добавку к весам модели, где на одну матрицу весов $W \\in \\mathbb{R}^{m{\\times}n} $ обучаются две низкоранговые матрицы $W_A \\in \\mathbb{R}^{k{\\times}n}$ и $W_B \\in \\mathbb{R}^{k{\\times}m}$, где $k$ - ранг матрицы сильно меньше $m$ и $n$.\n",
    "\n",
    "Тем самым, новая обученная матрица весов может быть представлена как $\\hat{W} = W + \\Delta W = W + W^T_B W_A$.  \n",
    "Во время инференса $\\Delta W$ можно вмержить в $W$ и получить итоговую модель. \n",
    "Также частая практика оставлять адаптеры как есть, чтобы была возможность для одной базовой модели учить несколько адаптеров под разные задачи и переключаться между ними по необходимости.\n",
    "\n",
    "Если не мержить адаптеры, то вычисления для линейного слоя происходят как на картинке ниже.\n",
    "\n",
    "<img src=https://storage.yandexcloud.net/yandex-research/cvweek-cd-task-images/lora-idea.jpg width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd911be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем к каким слоям модели мы будет добавлять адаптеры. Мы здесь указали типичный набор\n",
    "lora_modules = [\n",
    "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"proj_in\", \"proj_out\",\n",
    "    \"ff.net.0.proj\", \"ff.net.2\", \"conv1\", \"conv2\", \"conv_shortcut\",\n",
    "    \"downsamplers.0.conv\", \"upsamplers.0.conv\", \"time_emb_proj\"\n",
    "]\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # задает ранг у матриц A и B в LoRA.\n",
    "    lora_alpha=16, # контролирует LR с которым учим LoRA: lr_lora = lr_base * lora_alpha / r. Если учится только LoRA, то можно просто менять LR в оптимизаторе\n",
    "    target_modules=lora_modules\n",
    ")\n",
    "\n",
    "unet = pipe.unet\n",
    "\n",
    "# Создаем обертку исходной UNet модели с LoRA адаптерами, используя библиотеку PEFT\n",
    "unet = get_peft_model(unet, lora_config, adapter_name=\"rf\")\n",
    "\n",
    "# Включаем gradient checkpointing - важная техника для экономии памяти во время обучения\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Создаем оптимизатор\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f9673",
   "metadata": {},
   "source": [
    "## Цикл обучения\n",
    "\n",
    "### Задание 2 (1.5 балла)\n",
    "\n",
    "Ниже приведена почти готовая функция обучения модели. **Вам нужно реализовать**: \n",
    "* Подготовку входов для модели\n",
    "* Подсчет таргета и лосса\n",
    "* Реализация mixed-preciison обучения\n",
    "* Реалиpовать logit_normal сэмплирование шагов во время обучения. См. секцию 3.1 статье [SD3](https://arxiv.org/pdf/2403.03206) rf/lognorm(0, 1).\n",
    "\n",
    "Идея за logit_normal сэмплированием - чаще генерировать из середины интервала, где происходят наиболее сложно выучивываемые и значимые шаги. \n",
    "В статье [SD3](https://arxiv.org/pdf/2403.03206) сеттинг обучения rf/lognormal(0, 1) показывает лучшие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d7ac1",
   "metadata": {},
   "source": [
    "### Эффективное обучение\n",
    "Данное задание рассчитано на успешное выполнение на colab с бесплатной Tesla T4 c 15GB VRAM.\n",
    "Однако учить даже относительно небольшие T2I модели масштаба SD1.5 на коллабе в лоб проблематично.\n",
    "\n",
    "Для этого полезно применить ряд инженерных техник, чтобы уместиться в данный бюджет и учиться за разумное время.\n",
    "\n",
    "**Список техник**\n",
    "\n",
    "1) Включить **gradient checkpointing** для обучемой модели\n",
    "2) Добавить **LoRA** (Low Rank Adapters) адаптеры, чтобы учить не все веса, а только 10% добавочных весов\n",
    "3) Использовать **gradient accumulation**, чтобы делать итерацию обучения по бОльшему батчу, чем влезает по памяти\n",
    "4) Добавить **mixed precision** FP16/FP32 обучение модели для скорости. Обычно еще и память экономится, но в случае LoRA обучения + gradient checkpointing на память сильно влиять не должно, но зато станет быстрее.\n",
    "5) **Мульти-GPU** обучение - распределение вычислений по нескольким GPU.  \n",
    "\n",
    "**Что имеем на данный момент?**\n",
    "\n",
    "1-2) Уже сделано выше\n",
    "\n",
    "3 ) Можно реализовать, если по какой-то причине не влезаете по памяти, но поидее и без него все ок\n",
    "\n",
    "4 ) **Крайне полезно добавить в контексте скорости обучения** \n",
    "\n",
    "5 ) Недоступно, так как работаем на одной карточке\n",
    "\n",
    "### Mixed-precision обучение\n",
    "\n",
    "Про реализацию mixed-precision в pytorch можно перейти по ссылке: [Mixed-precision обучение](https://pytorch.org/docs/stable/notes/amp_examples.html#typical-mixed-precision-training).\n",
    "\n",
    "**Учтите**, что нас интересует именно FP16, а не BF16, так как последний не поддерживается на T4 карточках и профита не будет.\n",
    "\n",
    "**Совет** эта ячейка будет дублироваться в следующем ДЗ, так что в целях экономии времени лучше разобраться с mixed-precision пораньше, учитывая что реализация супер простая.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ff79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model,\n",
    "    pipe,\n",
    "    train_dataloader,\n",
    "    optimizer,\n",
    "    weighting_scheme='uniform',\n",
    "    device='cuda'\n",
    "):    \n",
    "    # Очищаем память GPU.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Инициализация mixed precision.\n",
    "    scaler = <YOUR CODE HERE>\n",
    "    \n",
    "    # Определяем интервал сигм для обучения\n",
    "    sigmas = torch.linspace(0, 0.999, 1000, device=device)\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        latents, prompt_embeds = prepare_batch(batch, pipe)\n",
    "        bsz = len(latents)\n",
    "        \n",
    "        if weighting_scheme == \"logit_normal\":\n",
    "            <YOUR CODE HERE>\n",
    "        else:\n",
    "            u = torch.rand(size=(bsz,), device=device)\n",
    "\n",
    "        # Конвертируем сигмы в шаги, которые будем подавать в модель\n",
    "        t = <YOUR CODE HERE>\n",
    "        noise = torch.randn_like(latents)\n",
    "        sigmas_t = extract_into_tensor(sigmas.cuda(), t, noise.shape)\n",
    "        \n",
    "        # Подготовте вход xt\n",
    "        xt = <YOUR CODE HERE>\n",
    "        \n",
    "        # with <YOUR CODE HERE>: # для реализации mixed-precision обучения\n",
    "        with <YOUR CODE HERE>:\n",
    "            pred_v = model(\n",
    "                xt,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                timestep=t,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "        \n",
    "        target_v = <YOUR CODE HERE>\n",
    "        loss = <YOUR CODE HERE>\n",
    "        \n",
    "        <YOUR CODE HERE> # Заменить подсчет градиентов и шаг оптимизатора с GradScaler-ом\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Печать лосса для мониторинга.\n",
    "        print(f\"Loss: {loss.detach().item():.6f}\")\n",
    "\n",
    "    # Очищаем память GPU.\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(\n",
    "    unet,\n",
    "    pipe,\n",
    "    train_dataloader,\n",
    "    optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff37edd",
   "metadata": {},
   "source": [
    "### Генерируем примеры с помощью нашей модели\n",
    "\n",
    "Подставляем функции **step_fn** и **schedule_fn** из **Части 1**. Можете использовать любой солвер и расписание на ваш вкус."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ysda_hw1.utils.sd15_inference import run\n",
    "\n",
    "def step_fn(latents, sigmas, v_pred, i, type_, v_pred_fn, cache=None):\n",
    "    pass\n",
    "\n",
    "\n",
    "def schedule_fn(n_points, type_, s=3):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebcc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A sad puppy with large eyes\"\n",
    "guidance_scale = 4.5\n",
    "\n",
    "solver_type = 'dpm2_multi'\n",
    "schedule_type = 'sd3'\n",
    "\n",
    "for num_inference_steps in [5, 10, 20]:\n",
    "    generator = torch.Generator('cuda').manual_seed(0)\n",
    "    \n",
    "    sigmas = schedule_fn(\n",
    "        n_points=num_inference_steps, \n",
    "        type_=schedule_type, \n",
    "        s=3\n",
    "    )\n",
    "    \n",
    "    images = run(\n",
    "        pipe,\n",
    "        prompt=prompt,\n",
    "        step_fn=functools.partial(step_fn, type_=solver_type),\n",
    "        sigmas=sigmas,\n",
    "        num_images_per_prompt=4,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8138a79",
   "metadata": {},
   "source": [
    "### Примеры промптов на поиграться\n",
    "\n",
    "\n",
    "    \"portrait photo of a girl, photograph, highly detailed face, depth of field, moody light, golden hour, style by Dan Winters, Steve McCurry, centered, extremely detailed, Nikon D850, award winning photography\"\n",
    "    \n",
    "    \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "    \n",
    "    'A girl with pale blue hair and a cami tank top'\n",
    "    \n",
    "    \"Four cows in a pen on a sunny day\"\n",
    "    \n",
    "    \"Three dogs sleeping together on an unmade bed\"\n",
    "    \n",
    "    \"A sky blue colored hippopotamus\"\n",
    "    \n",
    "    \"The interior of a mad scientists laboratory, Cluttered with science experiments, tools and strange machine, Eerie purple light, Close up, by Miyazaki\"\n",
    "    \n",
    "    \"a barred owl peeking out from dense tree branches\"\n",
    "    \n",
    "    \"a close-up of a blue dragonfly on a daffodil\"\n",
    "    \n",
    "    \"A green train is coming down the tracks\"\n",
    "    \"A photograph of the inside of a subway train. There are frogs sitting on the seats. One of them is reading a newspaper. The window shows the river in the background.\"\n",
    "    \n",
    "    \"a family of four posing at the Grand Canyon\"\n",
    "    \n",
    "    \"A high resolution photo of a donkey in a clown costume giving a lecture at the front of a lecture hall. The blackboard has mathematical equations on it. There are many students in the lecture hall.\"\n",
    "    \n",
    "    \"A castle made of tortilla chips, in a river made of salsa. There are tiny burritos walking around the castle\"\n",
    "    \n",
    "    \"A raccoon wearing formal clothes, wearing a tophat and holding a cane. The raccoon is holding a garbage bag. Oil painting in the style of abstract cubism.\"\n",
    "    \n",
    "    \"A castle made of cardboard.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d59d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
